{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train.csv data/test.csv data/dev.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DATA_DIR_PATH = 'data'\n",
    "TRAIN_PATH = os.path.join(DATA_DIR_PATH, 'train.csv')\n",
    "TEST_PATH = os.path.join(DATA_DIR_PATH, 'test.csv')\n",
    "DEV_PATH = os.path.join(DATA_DIR_PATH, 'dev.csv')\n",
    "print(TRAIN_PATH, TEST_PATH, DEV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = train_df[['sentence_1', 'source']].rename(columns={'sentence_1': 'sentence'})\n",
    "ts2 = train_df[['sentence_2', 'source']].rename(columns={'sentence_2': 'sentence'})\n",
    "ds1 = dev_df[['sentence_1', 'source']].rename(columns={'sentence_1': 'sentence'})\n",
    "ds2 = dev_df[['sentence_2', 'source']].rename(columns={'sentence_2': 'sentence'})\n",
    "train = pd.concat([ts1, ts2, ds1, ds2], ignore_index=True)\n",
    "\n",
    "fs1 = test_df[['sentence_1', 'source']].rename(columns={'sentence_1': 'sentence'})\n",
    "fs2 = test_df[['sentence_2', 'source']].rename(columns={'sentence_2': 'sentence'})\n",
    "test = pd.concat([fs1, fs2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['source'] = train['source'].str.replace('-rtt', '')\n",
    "train['source'] = train['source'].str.replace('-sampled', '')\n",
    "\n",
    "test['source'] = test['source'].str.replace('-rtt', '')\n",
    "test['source'] = test['source'].str.replace('-sampled', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ìŠ¤ë¦´ë„ìˆê³  ë°˜ì „ë„ ìˆê³  ì—¬ëŠ í•œêµ­ì˜í™” ì“°ë ˆê¸°ë“¤í•˜ê³ ëŠ” ì°¨ì›ì´ ë‹¤ë¥´ë„¤ìš”~</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì£¼íƒì²­ì•½ì¡°ê±´ ë³€ê²½í•´ì£¼ì„¸ìš”.</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì…ì‚¬í›„ ì²˜ìŒ ëŒ€ë©´ìœ¼ë¡œ ë§Œë‚˜ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ë¿Œë“¯ë¿Œë“¯ í•˜ë„¤ìš”!!</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19743</th>\n",
       "      <td>ì •ë§ ê°€ìŠ´ì„ ë”°ëœ»í•˜ê²Œ í•œ ì¢‹ì€ ë“œë¼ë§ˆ...</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19744</th>\n",
       "      <td>(ë¹„íƒ€ë¯¼ì„ ë¨¹ëŠ” ì¥ë©´)</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745</th>\n",
       "      <td>ë¬´ìŠ¨ì˜ë¯¸ë¡œ ë§Œë“¤ì—ˆëŠ”ì§€ ëª¨ë¥´ê² ìŒ..</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>(ì˜ˆ: ì£¼ë§ì—ëŠ” ê°œì¸ìº˜ë¦°ë”ë§Œ ë³´ê³ , ì—…ë¬´ì‹œê°„ì—ëŠ” ì—…ë¬´ìº˜ë¦°ë”ë§Œ ë³´ê¸°)</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19747</th>\n",
       "      <td>ì—‰ëš±í•œ ì˜ìƒë„ ëª‡ ê°œ ìˆìŠµë‹ˆë‹¤.</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19748 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sentence    source\n",
       "0      ìŠ¤ë¦´ë„ìˆê³  ë°˜ì „ë„ ìˆê³  ì—¬ëŠ í•œêµ­ì˜í™” ì“°ë ˆê¸°ë“¤í•˜ê³ ëŠ” ì°¨ì›ì´ ë‹¤ë¥´ë„¤ìš”~      nsmc\n",
       "1                        ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;     slack\n",
       "2                              ì£¼íƒì²­ì•½ì¡°ê±´ ë³€ê²½í•´ì£¼ì„¸ìš”.  petition\n",
       "3                      ì…ì‚¬í›„ ì²˜ìŒ ëŒ€ë©´ìœ¼ë¡œ ë§Œë‚˜ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.     slack\n",
       "4                                  ë¿Œë“¯ë¿Œë“¯ í•˜ë„¤ìš”!!     slack\n",
       "...                                       ...       ...\n",
       "19743                 ì •ë§ ê°€ìŠ´ì„ ë”°ëœ»í•˜ê²Œ í•œ ì¢‹ì€ ë“œë¼ë§ˆ...      nsmc\n",
       "19744                            (ë¹„íƒ€ë¯¼ì„ ë¨¹ëŠ” ì¥ë©´)     slack\n",
       "19745                      ë¬´ìŠ¨ì˜ë¯¸ë¡œ ë§Œë“¤ì—ˆëŠ”ì§€ ëª¨ë¥´ê² ìŒ..      nsmc\n",
       "19746   (ì˜ˆ: ì£¼ë§ì—ëŠ” ê°œì¸ìº˜ë¦°ë”ë§Œ ë³´ê³ , ì—…ë¬´ì‹œê°„ì—ëŠ” ì—…ë¬´ìº˜ë¦°ë”ë§Œ ë³´ê¸°)     slack\n",
       "19747                       ì—‰ëš±í•œ ì˜ìƒë„ ëª‡ ê°œ ìˆìŠµë‹ˆë‹¤.      nsmc\n",
       "\n",
       "[19748 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ê°€ìƒí™”íê±°ë˜ì†Œ íì‡„í•˜ì§€ ë§ê³ </td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë‡Œë¬¼ì í1í˜¸ 640ë§Œë‹¬ë¼ 70ì–µ ë‡Œë¬¼ë°›ì€ ê¶Œì–‘ìˆ™ êµ¬ì†í•˜ê³  ì¬ì‚°ì„ ëª°ìˆ˜í•˜ë¼</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê¸°ë¬´ì‚¬ ì˜ê´€ê¸‰ì˜ í•˜ê·¹ìƒ ì •ë§ ì´ëŒ€ë¡œ ë°©ê´€í•˜ëŠ”ê²Œ ë¯¼ì£¼ì£¼ì˜ ì¸ì§€ìš”</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>í™”ê¹Œì§€ê°€ í•œê³„ì˜€ë‹¤.</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì™œ í˜¼ì ìˆì§€.. ã… ã… </td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>ì˜¤ëŠ˜ ìŠ¬ë™ì˜ ì±„ë„ ë° ì‚¬ìš© ê¶Œí•œì— ëŒ€í•œ ë³€ê²½ì´ ìˆì„ ì˜ˆì •ì…ë‹ˆë‹¤!</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>ì²­ì†Œë…„ë³´í˜¸ë²• íì§€ ì²­ì›ì„œ</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>ì¹œì• í•˜ëŠ” ë¬¸ ëŒ€í†µë ¹ë‹˜, ë””ì ¤ì°¨ëŸ‰ ë‹¨ì†ì´ ë§¤ì—°ì„ ë‚´ëŠ” íŠ¸ëŸ­ìœ¼ë¡œ ì œí•œë˜ê¸°ë¥¼ ê°„ì ˆíˆ ê¸°ë„...</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>ìš”ì¦˜ ì¬ë¯¸ê°€ ì‚¬ë¼ì¡Œë‹¤...</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>ì¬ë°Œê²Œë´¤ìŠµë‹ˆë‹¤ ã…ã…</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence    source\n",
       "0                                       ê°€ìƒí™”íê±°ë˜ì†Œ íì‡„í•˜ì§€ ë§ê³   petition\n",
       "1              ë‡Œë¬¼ì í1í˜¸ 640ë§Œë‹¬ë¼ 70ì–µ ë‡Œë¬¼ë°›ì€ ê¶Œì–‘ìˆ™ êµ¬ì†í•˜ê³  ì¬ì‚°ì„ ëª°ìˆ˜í•˜ë¼  petition\n",
       "2                    ê¸°ë¬´ì‚¬ ì˜ê´€ê¸‰ì˜ í•˜ê·¹ìƒ ì •ë§ ì´ëŒ€ë¡œ ë°©ê´€í•˜ëŠ”ê²Œ ë¯¼ì£¼ì£¼ì˜ ì¸ì§€ìš”  petition\n",
       "3                                            í™”ê¹Œì§€ê°€ í•œê³„ì˜€ë‹¤.      nsmc\n",
       "4                                          ì™œ í˜¼ì ìˆì§€.. ã… ã…      slack\n",
       "...                                                 ...       ...\n",
       "2195                ì˜¤ëŠ˜ ìŠ¬ë™ì˜ ì±„ë„ ë° ì‚¬ìš© ê¶Œí•œì— ëŒ€í•œ ë³€ê²½ì´ ìˆì„ ì˜ˆì •ì…ë‹ˆë‹¤!     slack\n",
       "2196                                      ì²­ì†Œë…„ë³´í˜¸ë²• íì§€ ì²­ì›ì„œ  petition\n",
       "2197  ì¹œì• í•˜ëŠ” ë¬¸ ëŒ€í†µë ¹ë‹˜, ë””ì ¤ì°¨ëŸ‰ ë‹¨ì†ì´ ë§¤ì—°ì„ ë‚´ëŠ” íŠ¸ëŸ­ìœ¼ë¡œ ì œí•œë˜ê¸°ë¥¼ ê°„ì ˆíˆ ê¸°ë„...  petition\n",
       "2198                                     ìš”ì¦˜ ì¬ë¯¸ê°€ ì‚¬ë¼ì¡Œë‹¤...      nsmc\n",
       "2199                                         ì¬ë°Œê²Œë´¤ìŠµë‹ˆë‹¤ ã…ã…      nsmc\n",
       "\n",
       "[2200 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train['source_encoded'] = label_encoder.fit_transform(train['source'])\n",
    "test['source_encoded'] = label_encoder.transform(test['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "model = ElectraForSequenceClassification.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=len(label_encoder.classes_))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15798/15798 [00:05<00:00, 2716.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3950/3950 [00:01<00:00, 2817.94 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15798/15798 [00:01<00:00, 10181.05 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3950/3950 [00:00<00:00, 11134.25 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 1...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.216726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.173212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.186228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 evaluation: {'eval_loss': 0.18622753024101257, 'eval_runtime': 15.585, 'eval_samples_per_second': 253.449, 'eval_steps_per_second': 15.849, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15798/15798 [00:05<00:00, 2699.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3950/3950 [00:01<00:00, 2770.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15798/15798 [00:01<00:00, 10236.76 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3950/3950 [00:00<00:00, 11222.40 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.065006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.052049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.054223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 evaluation: {'eval_loss': 0.05422252416610718, 'eval_runtime': 15.5869, 'eval_samples_per_second': 253.418, 'eval_steps_per_second': 15.847, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15798/15798 [00:05<00:00, 2787.65 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3950/3950 [00:01<00:00, 2767.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15798/15798 [00:01<00:00, 8760.87 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3950/3950 [00:00<00:00, 11005.35 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 3...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.034038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.042841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.035648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 evaluation: {'eval_loss': 0.03564784303307533, 'eval_runtime': 15.5986, 'eval_samples_per_second': 253.227, 'eval_steps_per_second': 15.835, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15799/15799 [00:05<00:00, 2786.83 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3949/3949 [00:01<00:00, 2782.45 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15799/15799 [00:01<00:00, 8936.74 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3949/3949 [00:00<00:00, 11266.61 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 4...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.013665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.005606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.006182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 evaluation: {'eval_loss': 0.006181574426591396, 'eval_runtime': 15.5963, 'eval_samples_per_second': 253.202, 'eval_steps_per_second': 15.837, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15799/15799 [00:05<00:00, 2759.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3949/3949 [00:01<00:00, 2767.11 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15799/15799 [00:01<00:00, 9015.72 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3949/3949 [00:00<00:00, 11078.23 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.014150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.007316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.005027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 evaluation: {'eval_loss': 0.005026908591389656, 'eval_runtime': 15.6005, 'eval_samples_per_second': 253.132, 'eval_steps_per_second': 15.833, 'epoch': 3.0}\n",
      "Average kFold Accuracy: 0.0575\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.contiguous()\n",
    "\n",
    "# kFold ì„¤ì •\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# kFold êµì°¨ ê²€ì¦\n",
    "fold_accuracies = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n",
    "    # Train/Val ë°ì´í„°ë¥¼ ë¶„ë¦¬\n",
    "    train_data = train.iloc[train_idx]\n",
    "    val_data = train.iloc[val_idx]\n",
    "\n",
    "    # Hugging Face Datasets í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "    # í† í°í™” ì ìš©\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # í•„ìš”í•œ ì—´ë§Œ ì„ íƒ ë° ë¼ë²¨ì„ 'labels'ë¡œ ì„¤ì •\n",
    "    train_dataset = train_dataset.map(lambda examples: {'labels': examples['source_encoded']})\n",
    "    val_dataset = val_dataset.map(lambda examples: {'labels': examples['source_encoded']})\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{fold}',  # ê° í´ë“œë³„ë¡œ ê²°ê³¼ ì €ì¥\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,  # ì—í­ ìˆ˜\n",
    "        logging_dir=f'./logs_{fold}',\n",
    "        report_to=\"none\",  # ì½˜ì†”ì—ë§Œ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •\n",
    "        fp16=True,  # í˜¼í•© ì •ë°€ë„(16-bit floating point) ì‚¬ìš© (CUDAì—ì„œ ì„±ëŠ¥ í–¥ìƒ)\n",
    "        save_steps=1000,  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹ˆë„\n",
    "        save_total_limit=2  # ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ì˜ ìµœëŒ€ ìˆ˜\n",
    "    )\n",
    "\n",
    "    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    print(f\"Starting training for fold {fold+1}...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # í´ë“œë³„ í‰ê°€ ë° ì •í™•ë„ ê¸°ë¡\n",
    "    metrics = trainer.evaluate()\n",
    "    fold_accuracy = metrics['eval_loss']  # eval_lossë¥¼ ëŒ€ì‹  ì‚¬ìš©í•˜ê±°ë‚˜ eval_accuracyê°€ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì‚¬ìš©\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "    print(f\"Fold {fold+1} evaluation: {metrics}\")\n",
    "\n",
    "# ê° í´ë“œì˜ í‰ê·  ì •í™•ë„ ê³„ì‚°\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average kFold Accuracy: {average_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9559\n",
      "Test F1 Score: 0.9559\n"
     ]
    }
   ],
   "source": [
    "# ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€\n",
    "test_sentence_tokenized = tokenizer(list(test['sentence']), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(input_ids=test_sentence_tokenized['input_ids'], attention_mask=test_sentence_tokenized['attention_mask'])\n",
    "test_predictions = torch.argmax(test_outputs.logits, dim=-1)\n",
    "\n",
    "# ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ë¡œ Accuracy ë° F1 Score ê³„ì‚°\n",
    "true_labels = test['source_encoded'].values\n",
    "predictions = test_predictions.cpu().numpy()\n",
    "\n",
    "# ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€\n",
    "test_accuracy = accuracy_score(true_labels, predictions)\n",
    "test_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"saved/text_classifier_experiment_2.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petition']\n"
     ]
    }
   ],
   "source": [
    "example = \"ì œë°œ ì¢€ ëŒì•„ê°€ë¼\"\n",
    "token = tokenizer(example, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_predict_vector = model(input_ids=token['input_ids'], attention_mask=token['attention_mask'])\n",
    "example_predict = torch.argmax(example_predict_vector.logits, dim=-1)\n",
    "result = label_encoder.inverse_transform(example_predict.cpu())\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "level1-semantictextsimilarity-nlp-07-gjq2GaXo-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
