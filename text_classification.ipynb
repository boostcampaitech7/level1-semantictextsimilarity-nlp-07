{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train.csv data/test.csv data/dev.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DATA_DIR_PATH = 'data'\n",
    "TRAIN_PATH = os.path.join(DATA_DIR_PATH, 'train.csv')\n",
    "TEST_PATH = os.path.join(DATA_DIR_PATH, 'test.csv')\n",
    "DEV_PATH = os.path.join(DATA_DIR_PATH, 'dev.csv')\n",
    "print(TRAIN_PATH, TEST_PATH, DEV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = train_df[['sentence_1', 'source']].rename(columns={'sentence_1': 'sentence'})\n",
    "ts2 = train_df[['sentence_2', 'source']].rename(columns={'sentence_2': 'sentence'})\n",
    "ds1 = dev_df[['sentence_1', 'source']].rename(columns={'sentence_1': 'sentence'})\n",
    "ds2 = dev_df[['sentence_2', 'source']].rename(columns={'sentence_2': 'sentence'})\n",
    "train = pd.concat([ts1, ts2, ds1, ds2], ignore_index=True)\n",
    "\n",
    "fs1 = test_df[['sentence_1', 'source']].rename(columns={'sentence_1': 'sentence'})\n",
    "fs2 = test_df[['sentence_2', 'source']].rename(columns={'sentence_2': 'sentence'})\n",
    "test = pd.concat([fs1, fs2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['source'] = train['source'].str.replace('-rtt', '')\n",
    "train['source'] = train['source'].str.replace('-sampled', '')\n",
    "\n",
    "test['source'] = test['source'].str.replace('-rtt', '')\n",
    "test['source'] = test['source'].str.replace('-sampled', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19743</th>\n",
       "      <td>정말 가슴을 따뜻하게 한 좋은 드라마...</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19744</th>\n",
       "      <td>(비타민을 먹는 장면)</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745</th>\n",
       "      <td>무슨의미로 만들었는지 모르겠음..</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>(예: 주말에는 개인캘린더만 보고, 업무시간에는 업무캘린더만 보기)</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19747</th>\n",
       "      <td>엉뚱한 영상도 몇 개 있습니다.</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sentence    source\n",
       "0      스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~      nsmc\n",
       "1                        앗 제가 접근권한이 없다고 뜹니다;;     slack\n",
       "2                              주택청약조건 변경해주세요.  petition\n",
       "3                      입사후 처음 대면으로 만나 반가웠습니다.     slack\n",
       "4                                  뿌듯뿌듯 하네요!!     slack\n",
       "...                                       ...       ...\n",
       "19743                 정말 가슴을 따뜻하게 한 좋은 드라마...      nsmc\n",
       "19744                            (비타민을 먹는 장면)     slack\n",
       "19745                      무슨의미로 만들었는지 모르겠음..      nsmc\n",
       "19746   (예: 주말에는 개인캘린더만 보고, 업무시간에는 업무캘린더만 보기)     slack\n",
       "19747                       엉뚱한 영상도 몇 개 있습니다.      nsmc\n",
       "\n",
       "[19748 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>가상화폐거래소 폐쇄하지 말고</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>뇌물적폐1호 640만달라 70억 뇌물받은 권양숙 구속하고 재산을 몰수하라</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>기무사 영관급의 하극상 정말 이대로 방관하는게 민주주의 인지요</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>화까지가 한계였다.</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>왜 혼자 있지.. ㅠㅠ</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>오늘 슬랙의 채널 및 사용 권한에 대한 변경이 있을 예정입니다!</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>청소년보호법 폐지 청원서</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>친애하는 문 대통령님, 디젤차량 단속이 매연을 내는 트럭으로 제한되기를 간절히 기도...</td>\n",
       "      <td>petition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>요즘 재미가 사라졌다...</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>재밌게봤습니다 ㅎㅎ</td>\n",
       "      <td>nsmc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence    source\n",
       "0                                       가상화폐거래소 폐쇄하지 말고  petition\n",
       "1              뇌물적폐1호 640만달라 70억 뇌물받은 권양숙 구속하고 재산을 몰수하라  petition\n",
       "2                    기무사 영관급의 하극상 정말 이대로 방관하는게 민주주의 인지요  petition\n",
       "3                                            화까지가 한계였다.      nsmc\n",
       "4                                          왜 혼자 있지.. ㅠㅠ     slack\n",
       "...                                                 ...       ...\n",
       "2195                오늘 슬랙의 채널 및 사용 권한에 대한 변경이 있을 예정입니다!     slack\n",
       "2196                                      청소년보호법 폐지 청원서  petition\n",
       "2197  친애하는 문 대통령님, 디젤차량 단속이 매연을 내는 트럭으로 제한되기를 간절히 기도...  petition\n",
       "2198                                     요즘 재미가 사라졌다...      nsmc\n",
       "2199                                         재밌게봤습니다 ㅎㅎ      nsmc\n",
       "\n",
       "[2200 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train['source_encoded'] = label_encoder.fit_transform(train['source'])\n",
    "test['source_encoded'] = label_encoder.transform(test['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "model = ElectraForSequenceClassification.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=len(label_encoder.classes_))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15798/15798 [00:05<00:00, 2716.62 examples/s]\n",
      "Map: 100%|██████████| 3950/3950 [00:01<00:00, 2817.94 examples/s]\n",
      "Map: 100%|██████████| 15798/15798 [00:01<00:00, 10181.05 examples/s]\n",
      "Map: 100%|██████████| 3950/3950 [00:00<00:00, 11134.25 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 1...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.216726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.173212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.186228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 evaluation: {'eval_loss': 0.18622753024101257, 'eval_runtime': 15.585, 'eval_samples_per_second': 253.449, 'eval_steps_per_second': 15.849, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15798/15798 [00:05<00:00, 2699.81 examples/s]\n",
      "Map: 100%|██████████| 3950/3950 [00:01<00:00, 2770.86 examples/s]\n",
      "Map: 100%|██████████| 15798/15798 [00:01<00:00, 10236.76 examples/s]\n",
      "Map: 100%|██████████| 3950/3950 [00:00<00:00, 11222.40 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.065006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.052049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.054223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 evaluation: {'eval_loss': 0.05422252416610718, 'eval_runtime': 15.5869, 'eval_samples_per_second': 253.418, 'eval_steps_per_second': 15.847, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15798/15798 [00:05<00:00, 2787.65 examples/s]\n",
      "Map: 100%|██████████| 3950/3950 [00:01<00:00, 2767.64 examples/s]\n",
      "Map: 100%|██████████| 15798/15798 [00:01<00:00, 8760.87 examples/s] \n",
      "Map: 100%|██████████| 3950/3950 [00:00<00:00, 11005.35 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 3...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.034038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.042841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.035648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 evaluation: {'eval_loss': 0.03564784303307533, 'eval_runtime': 15.5986, 'eval_samples_per_second': 253.227, 'eval_steps_per_second': 15.835, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15799/15799 [00:05<00:00, 2786.83 examples/s]\n",
      "Map: 100%|██████████| 3949/3949 [00:01<00:00, 2782.45 examples/s]\n",
      "Map: 100%|██████████| 15799/15799 [00:01<00:00, 8936.74 examples/s] \n",
      "Map: 100%|██████████| 3949/3949 [00:00<00:00, 11266.61 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 4...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.013665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.005606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.006182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 evaluation: {'eval_loss': 0.006181574426591396, 'eval_runtime': 15.5963, 'eval_samples_per_second': 253.202, 'eval_steps_per_second': 15.837, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15799/15799 [00:05<00:00, 2759.86 examples/s]\n",
      "Map: 100%|██████████| 3949/3949 [00:01<00:00, 2767.11 examples/s]\n",
      "Map: 100%|██████████| 15799/15799 [00:01<00:00, 9015.72 examples/s] \n",
      "Map: 100%|██████████| 3949/3949 [00:00<00:00, 11078.23 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for fold 5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2964' max='2964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2964/2964 10:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.014150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.007316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.005027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 evaluation: {'eval_loss': 0.005026908591389656, 'eval_runtime': 15.6005, 'eval_samples_per_second': 253.132, 'eval_steps_per_second': 15.833, 'epoch': 3.0}\n",
      "Average kFold Accuracy: 0.0575\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.contiguous()\n",
    "\n",
    "# kFold 설정\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# kFold 교차 검증\n",
    "fold_accuracies = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n",
    "    # Train/Val 데이터를 분리\n",
    "    train_data = train.iloc[train_idx]\n",
    "    val_data = train.iloc[val_idx]\n",
    "\n",
    "    # Hugging Face Datasets 형식으로 변환\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "    # 토큰화 적용\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # 필요한 열만 선택 및 라벨을 'labels'로 설정\n",
    "    train_dataset = train_dataset.map(lambda examples: {'labels': examples['source_encoded']})\n",
    "    val_dataset = val_dataset.map(lambda examples: {'labels': examples['source_encoded']})\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # 학습 파라미터 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{fold}',  # 각 폴드별로 결과 저장\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,  # 에폭 수\n",
    "        logging_dir=f'./logs_{fold}',\n",
    "        report_to=\"none\",  # 콘솔에만 출력하도록 설정\n",
    "        fp16=True,  # 혼합 정밀도(16-bit floating point) 사용 (CUDA에서 성능 향상)\n",
    "        save_steps=1000,  # 체크포인트 저장 빈도\n",
    "        save_total_limit=2  # 저장할 체크포인트의 최대 수\n",
    "    )\n",
    "\n",
    "    # 트레이너 설정\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    print(f\"Starting training for fold {fold+1}...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 폴드별 평가 및 정확도 기록\n",
    "    metrics = trainer.evaluate()\n",
    "    fold_accuracy = metrics['eval_loss']  # eval_loss를 대신 사용하거나 eval_accuracy가 있으면 그 값을 사용\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "    print(f\"Fold {fold+1} evaluation: {metrics}\")\n",
    "\n",
    "# 각 폴드의 평균 정확도 계산\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average kFold Accuracy: {average_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9559\n",
      "Test F1 Score: 0.9559\n"
     ]
    }
   ],
   "source": [
    "# 최종 테스트 데이터 평가\n",
    "test_sentence_tokenized = tokenizer(list(test['sentence']), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(input_ids=test_sentence_tokenized['input_ids'], attention_mask=test_sentence_tokenized['attention_mask'])\n",
    "test_predictions = torch.argmax(test_outputs.logits, dim=-1)\n",
    "\n",
    "# 실제 라벨과 예측 라벨로 Accuracy 및 F1 Score 계산\n",
    "true_labels = test['source_encoded'].values\n",
    "predictions = test_predictions.cpu().numpy()\n",
    "\n",
    "# 최종 테스트 데이터에 대한 성능 평가\n",
    "test_accuracy = accuracy_score(true_labels, predictions)\n",
    "test_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"saved/text_classifier_experiment_2.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petition']\n"
     ]
    }
   ],
   "source": [
    "example = \"제발 좀 돌아가라\"\n",
    "token = tokenizer(example, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_predict_vector = model(input_ids=token['input_ids'], attention_mask=token['attention_mask'])\n",
    "example_predict = torch.argmax(example_predict_vector.logits, dim=-1)\n",
    "result = label_encoder.inverse_transform(example_predict.cpu())\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "level1-semantictextsimilarity-nlp-07-gjq2GaXo-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
