{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.config.data_loader_config import DATA_LOADER_CONFIG, OPTIMIZER_CONFIG\n",
    "from src.data_loader.loader import Dataloader\n",
    "from src.model.model import Model, Models, LossFunctions\n",
    "from src.trainer.predict import save_result\n",
    "import src.callback as callback\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "# Parameters 설정\n",
    "batch_size = DATA_LOADER_CONFIG['batch_size']\n",
    "shuffle = DATA_LOADER_CONFIG['shuffle']\n",
    "learning_rate = OPTIMIZER_CONFIG['learning_rate']\n",
    "max_epoch = OPTIMIZER_CONFIG['max_epoch']\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "num_workers: int = DATA_LOADER_CONFIG.get('num_workers', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 선언\n",
    "# select Model -> roberta_base, roberta_small, roberta_large, electra_base, electra_base_v3\n",
    "model_name = 'electra_base_v3'\n",
    "model = Model(Models.electra_base_v3, learning_rate, LossFunctions.hu_loss)\n",
    "\n",
    "# Dataloader 선언\n",
    "dataloader = Dataloader(batch_size, shuffle, Models.electra_base_v3, check_aug=True)\n",
    "\n",
    "# callback 정의\n",
    "epoch_print_callback = callback.EpochPrintCallback()\n",
    "checkpoint_callback = callback.ModelCheckpoint(model_name=model_name)\n",
    "early_stopping = callback.EarlyStopping()\n",
    "lr_monitor = callback.LearningRateMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/data/ephemeral/home/level1-semantictextsimilarity-nlp-07/src/preprocessing/preprocessor.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_zero_labels[['sentence_1', 'sentence_2']] = non_zero_labels[['sentence_2', 'sentence_1']]\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /data/ephemeral/home/level1-semantictextsimilarity-nlp-07/electra_base_v3/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | plm       | ElectraForSequenceClassification | 112 M \n",
      "1 | loss_func | HuberLoss                        | 0     \n",
      "---------------------------------------------------------------\n",
      "112 M     Trainable params\n",
      "0         Non-trainable params\n",
      "112 M     Total params\n",
      "451.688   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:993: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n",
      "/opt/conda/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float16).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1033/1033 [00:57<00:00, 17.84it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:993: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1033/1033 [00:58<00:00, 17.57it/s, v_num=8]Epoch 0 ended\n",
      "Validation Loss: 0.8524\n",
      "Training Loss: 1.1511\n",
      "Validation Pearson Correlation: 0.5748\n",
      "----------------------------------------\n",
      "Epoch 0: 100%|██████████| 1033/1033 [00:58<00:00, 17.57it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:993: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n",
      "Metric val_loss improved. New best score: 0.852\n",
      "Epoch 0, global step 1033: 'val_loss' reached 0.85244 (best 0.85244), saving model to '/data/ephemeral/home/level1-semantictextsimilarity-nlp-07/electra_base_v3/checkpoints/best-model_name=0-epoch=00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1033/1033 [00:58<00:00, 17.51it/s, v_num=8]Epoch 1 ended\n",
      "Validation Loss: 0.8434\n",
      "Training Loss: 0.7482\n",
      "Validation Pearson Correlation: 0.8259\n",
      "----------------------------------------\n",
      "Epoch 1: 100%|██████████| 1033/1033 [00:59<00:00, 17.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.843\n",
      "Epoch 1, global step 2066: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1033/1033 [00:58<00:00, 17.54it/s, v_num=8]Epoch 2 ended\n",
      "Validation Loss: 0.8861\n",
      "Training Loss: 0.9342\n",
      "Validation Pearson Correlation: 0.8678\n",
      "----------------------------------------\n",
      "Epoch 2: 100%|██████████| 1033/1033 [00:58<00:00, 17.54it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3099: 'val_loss' reached 0.88609 (best 0.88609), saving model to '/data/ephemeral/home/level1-semantictextsimilarity-nlp-07/electra_base_v3/checkpoints/best-model_name=0-epoch=02-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1033/1033 [00:59<00:00, 17.48it/s, v_num=8]Epoch 3 ended\n",
      "Validation Loss: 0.8737\n",
      "Training Loss: 1.0542\n",
      "Validation Pearson Correlation: 0.8755\n",
      "----------------------------------------\n",
      "Epoch 3: 100%|██████████| 1033/1033 [00:59<00:00, 17.48it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4132: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1033/1033 [00:58<00:00, 17.62it/s, v_num=8]Epoch 4 ended\n",
      "Validation Loss: 0.8696\n",
      "Training Loss: 0.7315\n",
      "Validation Pearson Correlation: 0.8592\n",
      "----------------------------------------\n",
      "Epoch 4: 100%|██████████| 1033/1033 [00:58<00:00, 17.62it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5165: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1033/1033 [00:58<00:00, 17.62it/s, v_num=8]Epoch 5 ended\n",
      "Validation Loss: 0.8654\n",
      "Training Loss: 0.7945\n",
      "Validation Pearson Correlation: 0.8590\n",
      "----------------------------------------\n",
      "Epoch 5: 100%|██████████| 1033/1033 [00:58<00:00, 17.62it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6198: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1033/1033 [00:59<00:00, 17.46it/s, v_num=8]Epoch 6 ended\n",
      "Validation Loss: 0.8496\n",
      "Training Loss: 1.1913\n",
      "Validation Pearson Correlation: 0.8054\n",
      "----------------------------------------\n",
      "Epoch 6: 100%|██████████| 1033/1033 [00:59<00:00, 17.46it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7231: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1033/1033 [00:59<00:00, 17.47it/s, v_num=8]Epoch 7 ended\n",
      "Validation Loss: 0.8399\n",
      "Training Loss: 0.9407\n",
      "Validation Pearson Correlation: 0.8257\n",
      "----------------------------------------\n",
      "Epoch 7: 100%|██████████| 1033/1033 [00:59<00:00, 17.47it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.840\n",
      "Epoch 7, global step 8264: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1033/1033 [00:58<00:00, 17.61it/s, v_num=8]Epoch 8 ended\n",
      "Validation Loss: 0.8591\n",
      "Training Loss: 1.0255\n",
      "Validation Pearson Correlation: 0.8374\n",
      "----------------------------------------\n",
      "Epoch 8: 100%|██████████| 1033/1033 [00:58<00:00, 17.61it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9297: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1033/1033 [00:59<00:00, 17.49it/s, v_num=8]Epoch 9 ended\n",
      "Validation Loss: 0.8542\n",
      "Training Loss: 0.9178\n",
      "Validation Pearson Correlation: 0.8368\n",
      "----------------------------------------\n",
      "Epoch 9: 100%|██████████| 1033/1033 [00:59<00:00, 17.49it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10330: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1033/1033 [00:59<00:00, 17.51it/s, v_num=8]Epoch 10 ended\n",
      "Validation Loss: 0.8686\n",
      "Training Loss: 0.9667\n",
      "Validation Pearson Correlation: 0.8404\n",
      "----------------------------------------\n",
      "Epoch 10: 100%|██████████| 1033/1033 [00:59<00:00, 17.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 11363: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  52%|█████▏    | 536/1033 [00:30<00:27, 17.78it/s, v_num=8] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator='gpu', devices='auto', max_epochs=max_epoch, callbacks=[lr_monitor, epoch_print_callback,checkpoint_callback, early_stopping], precision='16-mixed')\n",
    "trainer.fit(model=model, datamodule=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 가장 좋은 모델 불러오기\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241m.\u001b[39mbest_model_path\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mload_from_checkpoint(best_model_path, loss_func\u001b[38;5;241m=\u001b[39mLossFunctions\u001b[38;5;241m.\u001b[39mhu_loss)\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model\u001b[38;5;241m=\u001b[39mmodel, datamodule\u001b[38;5;241m=\u001b[39mdataloader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_callback' is not defined"
     ]
    }
   ],
   "source": [
    "# 가장 좋은 모델 불러오기\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = Model.load_from_checkpoint(best_model_path, loss_func=LossFunctions.hu_loss)\n",
    "trainer.test(model=model, datamodule=dataloader)\n",
    "# 추론\n",
    "predictions = trainer.predict(model=model, datamodule=dataloader)\n",
    "\n",
    "# 결과 저장\n",
    "save_result(predictions, model_name, max_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
