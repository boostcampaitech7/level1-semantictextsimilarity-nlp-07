{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.config.data_loader_config import DATA_LOADER_CONFIG, OPTIMIZER_CONFIG\n",
        "from src.data_loader.loader import Dataloader\n",
        "from src.model.model import Model, Models, LossFunctions\n",
        "from src.trainer.predict import save_result\n",
        "from utils.fix_seed import set_seed\n",
        "import src.callback as callback\n",
        "import pytorch_lightning as pl\n",
        "import os\n",
        "\n",
        "# Parameters 설정\n",
        "batch_size = DATA_LOADER_CONFIG['batch_size']\n",
        "shuffle = DATA_LOADER_CONFIG['shuffle']\n",
        "learning_rate = OPTIMIZER_CONFIG['learning_rate']\n",
        "max_epoch = OPTIMIZER_CONFIG['max_epoch']\n",
        "num_workers: int = DATA_LOADER_CONFIG.get('num_workers', 4)  # num_workers 기본값 4로 설정\n",
        "\n",
        "set_seed(0)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "num_workers: int = DATA_LOADER_CONFIG.get('num_workers', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 선언\n",
        "# select Model -> roberta_base, roberta_small, roberta_large, electra_base, electra_base_v3\n",
        "model_name = 'synatra7b'\n",
        "model = Model(Models.model_dict[model_name], learning_rate, LossFunctions.hu_loss).half()\n",
        "\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(Models.model_dict[model_name])\n",
        "\n",
        "# 패딩 토큰 설정\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 패딩을 적용하는 데이터 콜레이터 설정\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "# Dataloader 선언\n",
        "dataloader = Dataloader(batch_size, shuffle, Models.model_dict[model_name], check_aug=False, num_workers=num_workers)\n",
        "\n",
        "# callback 정의\n",
        "epoch_print_callback = callback.EpochPrintCallback()\n",
        "checkpoint_callback = callback.ModelCheckpoint(model_name=model_name)\n",
        "early_stopping = callback.EarlyStopping()\n",
        "lr_monitor = callback.LearningRateMonitor()\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator='auto',\n",
        "    devices='auto',\n",
        "    max_epochs=max_epoch,\n",
        "    callbacks=[lr_monitor, epoch_print_callback, checkpoint_callback, early_stopping],\n",
        "    precision='16-mixed',\n",
        "    deterministic=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=max_epoch, callbacks=[lr_monitor, epoch_print_callback,checkpoint_callback, early_stopping])\n",
        "trainer.fit(model=model, datamodule=dataloader)\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tokenizer parallelism = false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 추론\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback.best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback.best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 가장 좋은 모델 불러오기\n",
        "best_model_path = checkpoint_callback.best_model_path\n",
        "model = Model.load_from_checkpoint(best_model_path, loss_func=LossFunctions.hu_loss)\n",
        "trainer.test(model=model, datamodule=dataloader)\n",
        "# 추론\n",
        "predictions = trainer.predict(model=model, datamodule=dataloader, precision = '16-mixed')\n",
        "\n",
        "# 결과 저장\n",
        "save_result(predictions, model_name, max_epoch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
